\documentclass[varwidth, border=20pt]{standalone}
\usepackage{algpseudocode}
\usepackage{algorithmicx}  
\usepackage{amsmath}
\usepackage{microtype}      
\usepackage{caption}
\usepackage{graphicx}


\captionsetup{labelfont=bf, font=small}
\renewcommand{\algorithmiccomment}[1]{\hfill\(\triangleright\) #1}
\newcommand{\algspace}{\vspace{3pt}}

\begin{document}

\begingroup
\small                    
\begin{center}
  \begin{minipage}{0.99\linewidth} 
    \hrule height 0.7pt
    \vspace{3pt}
    Algorithm: COMA
    \vspace{3pt}
    \hrule height 0.7pt
    \vspace{6pt}

   
    \begin{algorithmic}[1]
      \State Initialize: $\theta$ the parameters of individual policies $pi_i(.;\theta)$, $\phi$ the parameters of the centralized Q-network $Q(;\phi)$, and $\phi^-$ the parameters the target Q network. 
      \While{$t < T$}
        \State Initialize a rollout buffer $\mathcal{D}$ // $(episode_1, episode_2, \dots)$
        \For{ a number of episodes}:
            \State $current\_episode = \{ \}$ 
                \While{ $\mathbf{o}_t$ is not $done$}
                \State Collect observations $\{o^t_1,\dots,o^t_n\}$ and state $\mathbf{s}^t$
                \State  Sample an action $a_i^t \sim \pi_i(.|o_i^t)$ for each agent $i$
                \State Execute joint action $\mathbf{a}^t = (a_1^t,\dots,a_n^t)$
                \State Collect $r^t$, $done^t$
                \State Store $(\mathbf{s}^t,\mathbf{o}^t,\mathbf{a}^t,r^t,done^t,\mathbf{o}^{t+1})$ in $current\_episode$
                \EndWhile
            \State Store $current\_episode$ in the rollout buffer $\mathcal{D}$
        \EndFor
        \State Process the rollout buffer for batch training // Episodes with different lengths
        \State Train the centralized critic using TD($\lambda$)
        \Statex
          \[
            \mathcal{L}(\theta)=\tfrac{1}{|\mathcal{B}|}\sum_b \tfrac{1}{L^b} \sum_t \Big(y^{t,b}- Q^{tot}(\mathbf{s}^{t,b},\mathbf{o}^{t,b},\mathbf{a}^{t,b}; \phi)\Big)^2
          \]
        \State Every $C$ training steps, update $\phi^- \leftarrow \phi$ //training step = one full buffer pass
        \State Compute the counterfactual advantages
         \[
            A_i(\mathbf{s}, \mathbf{o},\mathbf{a}) = Q(\mathbf{s}, \mathbf{o},\mathbf{a};\phi) - \sum_{a'_i} \pi_i(a'_i|o_i;\theta) Q(\mathbf{s}, \mathbf{o},(\mathbf{a}_{-i},a'_i);\phi)
         \]

         \State Perform a gradient descent using:
         \[ 
             - \sum_i A_i(\mathbf{s}, \mathbf{o},\mathbf{a}) \log(\pi(a_i,o_i;\theta))
         \]

      \EndWhile
    \end{algorithmic}

    \vspace{6pt}
    \hrule height 0.7pt
  \end{minipage}
\end{center}
\endgroup

\end{document}
