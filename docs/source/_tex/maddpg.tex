\documentclass[varwidth, border=20pt]{standalone}
\usepackage{algpseudocode}
\usepackage{algorithmicx}  
\usepackage{amsmath}
\usepackage{microtype}      
\usepackage{caption}
\usepackage{graphicx}


\captionsetup{labelfont=bf, font=small}
\renewcommand{\algorithmiccomment}[1]{\hfill\(\triangleright\) #1}
\newcommand{\algspace}{\vspace{3pt}}

\begin{document}

\begingroup
\small                    
\begin{center}
  \begin{minipage}{0.99\linewidth} 
    \hrule height 0.7pt
    \vspace{3pt}
    Algorithm: MADDPG
    \vspace{3pt}
    \hrule height 0.7pt
    \vspace{6pt}

   
    \begin{algorithmic}[1]
      \State Initialize: $\theta$ and  $\theta^-$ the parameters of individual policy and target policy $\mu_i$, $\phi$ and $\phi^-$ the parameters central Q-function $Q$.
      \State Initialize replay buffer $\mathcal{D}$ // $(episode_1, episode_2, \dots)$
      \While{$t < T$}
        \State $current\_episode = \{ \}$ 
        \While{ $\mathbf{o}_t$ is not $done$}
        \State Collect observations $\{o^t_1,\dots,o^t_n\}$ and state $\mathbf{s}_t$
        \State Select an action $a^t_i = \mu(o^t_i)$ for each agent $i$. 
         \State Execute the joint action $\mathbf{a}^t = (a_1^t,\dots,a_n^t)$
         \State Collect $r^t$, $done^t$
         \State Store $(\mathbf{s}_t,\mathbf{o}^t,\mathbf{a}^t,r^t,done^t)$ in $current\_episode$
        \EndWhile
        \State Store $current\_episode$ in the replay buffer $\mathcal{D}$
        
        \If{$t$ is a training step}
          \State Sample batch of episodes 
          \[ 
            \mathcal{B} = \{ \{\mathbf{s}^{t,b},\mathbf{o}^{t,b},\mathbf{a}^{t,b},r^{t,b},done^{t,b},\mathbf{s'}^{t,b},\mathbf{o'}^{t,b}\}_{t=1 ... L^b}\}_{b=1, ..., |\mathcal{B}|}\]
          \State Set the targets 
          \Statex
          \begin{align*}
            y^{t,b} &= r^{t,b} +  (1 - done^{t,b}) Q(\mathbf{s'^{t,b}},\mu(o'^{t,b}_1;\theta^-), \dots , \mu(o'^{t,b}_n;\theta^-); \phi^-) 
            \end{align*}
            
          \State Update $\phi$ using :
          \Statex
          \[
            \mathcal{L}(\phi)=\tfrac{1}{|\mathcal{B}|}\sum_b \tfrac{1}{L^b} \sum_t \Big(y^{t,b}- Q(\mathbf{s}^{t,b},a^{t,b}_1, \dots , a^{t,b}_n; \phi)\Big)^2
          \]
          \State Update $\theta$ using: 
          \[
            \mathcal{L}(\theta) = - \tfrac{1}{|\mathcal{B}|}\sum_b \tfrac{1}{L^b} \sum_t \sum_i Q(\mathbf{s}^{t,b},a^{t,b}_1, \dots, \mu(o^{t,b}_i;\theta), \dots , a^{t,b}_n;\phi)
          \]
          \State Every $C$ steps, update $\theta^- \leftarrow \theta$, $\phi^- \leftarrow \phi$ 
        \EndIf
      \EndWhile
    \end{algorithmic}

    \vspace{6pt}
    \hrule height 0.7pt
  \end{minipage}
\end{center}
\endgroup

\end{document}
